<!DOCTYPE html>
<html lang="en">

<body>

<p>
<a href="https://jaffulee.github.io/Jaffulee/">
  Visit my website
</a>
</p>

<hr>

<h1>AI-Based Vehicle Tracking, Counting, and Lane Estimation</h1>

<p>
This project implements a complete <strong>computer vision pipeline</strong> for
<strong>vehicle detection, tracking, motion analysis, and lane estimation</strong>
from real-world road traffic video.
</p>

<p>
It combines:
</p>

<ul>
  <li>Deep-learning-based object detection and tracking (YOLO)</li>
  <li>Custom temporal continuity logic for ID stability</li>
  <li>Velocity and direction estimation in pixel-space</li>
  <li>Event-based counting via spatial border crossings</li>
  <li>Unsupervised lane estimation using clustering geometry</li>
  <li>Structured analytics outputs suitable for downstream BI or data science</li>
</ul>

<hr>

<h2>Tracked Output Examples</h2>

<p>
Below are example outputs generated by the pipeline, showing:
</p>

<ul>
  <li>Persistent vehicle IDs</li>
  <li>Velocity vectors drawn per vehicle</li>
  <li>Border-based entry / exit detection</li>
</ul>

<video controls width="800">
  <source src="output_video/road_video_tracked.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<br><br>

<video controls width="800">
  <source src="output_video/road_video_2_tracked.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<hr>

<p>
For convenience, trimmed versions are included directly in the repository:
</p>

<ul>
  <li><code>data/road_video.mp4</code></li>
  <li><code>data/road_video_2.mp4</code></li>
</ul>

<hr>
<h2>Sample Input Videos</h2>

<p>
The following public road traffic videos are used as example inputs:
</p>

<ul>
  <li>
    <a href="https://www.youtube.com/watch?v=MNn9qKG2UFI">
      Road Traffic Footage – Video 1 (YouTube)
    </a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=4aWufTZDLMU">
      Road Traffic Footage – Video 2 (YouTube)
    </a>
  </li>
</ul>

<hr>

<h2>Pipeline Overview</h2>

<h3>1. Detection & Tracking</h3>

<ul>
  <li>Uses <strong>YOLOv10</strong> with built-in tracking support.</li>
  <li>Each detected vehicle is assigned a persistent ID.</li>
  <li>Fallback IDs are generated if the model temporarily drops tracking.</li>
</ul>

<h3>2. Temporal Continuity & Stability</h3>

<ul>
  <li>Custom overlap clustering resolves same-frame detection conflicts.</li>
  <li>Vehicle positions are extrapolated using constant-velocity prediction.</li>
  <li>Configurable stability controls:
    <ul>
      <li>Maximum extrapolation distance</li>
      <li>Maximum frame gaps</li>
      <li>Velocity spike rejection</li>
      <li>EMA velocity smoothing</li>
    </ul>
  </li>
</ul>

<h3>3. Motion Analysis</h3>

<ul>
  <li>Velocity computed in pixel-space per frame.</li>
  <li>Direction vectors normalised and visualised.</li>
  <li>Speed stored both per-frame and per-second.</li>
</ul>

<h3>4. Event-Based Counting</h3>

<ul>
  <li>An inner border defines a counting region.</li>
  <li>Vehicles are counted on border state changes (In / Out).</li>
  <li>Each vehicle contributes only one entry and one exit event.</li>
</ul>

<h3>5. Lane Estimation (Unsupervised)</h3>

<ul>
  <li>Uses event center points of vehicles entering or leaving.</li>
  <li>KMeans clustering applied to spatial coordinates.</li>
  <li>The number of lanes is estimated via:
    <ul>
      <li>Inertia curvature (elbow method)</li>
      <li>Optional log-inertia stabilisation</li>
      <li>Quadratic refinement allowing non-integer estimates</li>
    </ul>
  </li>
</ul>

<hr>

<h2>Outputs</h2>

<p>
For each video, the pipeline produces:
</p>

<ul>
  <li><strong>Tracked video</strong> (<code>output_video/</code>)</li>
  <li><strong>Event-level fact table</strong></li>
  <li><strong>Video-level dimension table</strong></li>
  <li><strong>Aggregated traffic metrics</strong></li>
</ul>

<p>
Across all videos, unified datasets are written to:
</p>

<pre>
output_data/all_videos/
  ├── Cars_detected.csv
  ├── Cars_detected_fact.csv
  ├── Cars_detected_dim.csv
  └── Cars_detected_agg.csv
</pre>

These outputs are structured for easy ingestion into SQL, BI tools, or further Python analysis.

<hr>

<h2>Related Projects</h2>

<p>
This repository is part of a broader body of work exploring
<strong>AI, optimisation, and mathematical modelling</strong>.
</p>

<ul>
  <li>
    <strong>Gradient Descent Optimiser & Neural Network (From First Principles)</strong><br>
    A fully custom neural network framework implemented from scratch using
    tensor-based backpropagation and generalised Jacobians.<br>
    <a href="https://github.com/Jaffulee/deep_neural_network">
      View repository
    </a>
  </li>
</ul>

<p>
Together, these projects demonstrate both:
</p>

<ul>
  <li>Low-level mathematical derivation of learning algorithms.</li>
  <li>End-to-end applied AI systems operating on real-world data.</li>
</ul>

<hr>

<h2>Summary</h2>

<p>
This project demonstrates:
</p>

<ul>
  <li>Robust vehicle tracking under real-world conditions.</li>
  <li>Motion estimation without camera calibration.</li>
  <li>Event-driven traffic counting.</li>
  <li>Unsupervised lane estimation using geometric structure.</li>
  <li>Production-style data outputs suitable for analytics pipelines.</li>
</ul>

</body>
</html>
